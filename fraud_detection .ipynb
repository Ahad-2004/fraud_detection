{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oet99pt9GyhX"
      },
      "source": [
        "  \n",
        "# *final model*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCX9iVdQgbmY"
      },
      "source": [
        "# please upload test_2021.csv and training data.csv from github  [https://github.com/Ahad-2004/fraud_detection]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zci0d_3L8QfX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "ByHINIUp8QdK",
        "outputId": "32b9803b-8f96-441d-d3af-7047ba1df262"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install flask flask-cors scikit-learn pandas numpy xgboost joblib imbalanced-learn lightgbm\n",
        "\n",
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "import joblib\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def load_and_preprocess_data():\n",
        "    \"\"\"\n",
        "    Load and preprocess the fraud detection dataset\n",
        "    \"\"\"\n",
        "    print(\"Loading training data...\")\n",
        "    try:\n",
        "        # Load your training data\n",
        "        df = pd.read_csv('/content/training data.csv')\n",
        "        print(f\"Loaded training dataset with shape: {df.shape}\")\n",
        "        print(f\"Columns: {list(df.columns)}\")\n",
        "\n",
        "        # The last column in your dataset appears to be the target variable\n",
        "        target_column = df.columns[-1]  # Last column as target\n",
        "        print(f\"Identified target column: {target_column}\")\n",
        "\n",
        "        # Check the target column values\n",
        "        print(f\"Target column unique values:\\n{df[target_column].value_counts()}\")\n",
        "        print(f\"Fraud rate: {df[target_column].mean():.3f}\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(\"Training data file not found at /content/training data.csv\")\n",
        "        return None, None\n",
        "\n",
        "    return df, target_column\n",
        "\n",
        "def preprocess_data(df, target_column):\n",
        "    \"\"\"\n",
        "    Preprocess the data for model training\n",
        "    \"\"\"\n",
        "    # Remove claim_number as it's likely an identifier\n",
        "    if 'claim_number' in df.columns:\n",
        "        df = df.drop('claim_number', axis=1)\n",
        "\n",
        "    # Convert date column to numerical features\n",
        "    if 'claim_date' in df.columns:\n",
        "        df['claim_date'] = pd.to_datetime(df['claim_date'], errors='coerce')\n",
        "        df['claim_year'] = df['claim_date'].dt.year\n",
        "        df['claim_month'] = df['claim_date'].dt.month\n",
        "        df['claim_day'] = df['claim_date'].dt.day\n",
        "        df = df.drop('claim_date', axis=1)\n",
        "\n",
        "    # Separate features and target\n",
        "    X = df.drop([target_column], axis=1)\n",
        "    y = df[target_column]\n",
        "\n",
        "    print(f\"Features shape: {X.shape}\")\n",
        "    print(f\"Target shape: {y.shape}\")\n",
        "\n",
        "    # Identify categorical columns (non-numeric)\n",
        "    categorical_columns = X.select_dtypes(include=['object']).columns\n",
        "    label_encoders = {}\n",
        "\n",
        "    print(f\"Categorical columns to encode: {list(categorical_columns)}\")\n",
        "\n",
        "    # Encode categorical variables\n",
        "    for col in categorical_columns:\n",
        "        le = LabelEncoder()\n",
        "        X[col] = le.fit_transform(X[col].astype(str))\n",
        "        label_encoders[col] = le\n",
        "        print(f\"Encoded {col}: {le.classes_}\")\n",
        "\n",
        "    # Handle any remaining non-numeric columns by converting to numeric\n",
        "    for col in X.columns:\n",
        "        if not pd.api.types.is_numeric_dtype(X[col]):\n",
        "            X[col] = pd.to_numeric(X[col], errors='coerce')\n",
        "\n",
        "    # Fill any remaining NaN values\n",
        "    X = X.fillna(X.mean())\n",
        "\n",
        "    # Save label encoders\n",
        "    joblib.dump(label_encoders, 'label_encoders.pkl')\n",
        "\n",
        "    return X, y, label_encoders\n",
        "\n",
        "def train_best_model(X_train, y_train, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Train the best performing model with hyperparameter tuning\n",
        "    \"\"\"\n",
        "    print(\"Training XGBoost model with hyperparameter tuning...\")\n",
        "\n",
        "    # Define parameter grid for XGBoost\n",
        "    param_grid = {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [6, 8, 10],\n",
        "        'learning_rate': [0.05, 0.1, 0.15],\n",
        "        'subsample': [0.8, 0.9, 1.0],\n",
        "        'colsample_bytree': [0.8, 0.9, 1.0],\n",
        "        'min_child_weight': [1, 3, 5]\n",
        "    }\n",
        "\n",
        "    # Calculate scale_pos_weight for handling class imbalance\n",
        "    scale_pos_weight = len(y_train[y_train == 0]) / len(y_train[y_train == 1])\n",
        "\n",
        "    # Create XGBoost classifier with class imbalance handling\n",
        "    xgb_model = xgb.XGBClassifier(\n",
        "        random_state=42,\n",
        "        eval_metric='auc',\n",
        "        scale_pos_weight=scale_pos_weight\n",
        "    )\n",
        "\n",
        "    # Perform grid search with cross-validation\n",
        "    print(\"Performing hyperparameter tuning...\")\n",
        "    grid_search = GridSearchCV(\n",
        "        estimator=xgb_model,\n",
        "        param_grid=param_grid,\n",
        "        scoring='roc_auc',\n",
        "        cv=3,  # Reduced for faster training\n",
        "        n_jobs=-1,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    # Get the best model\n",
        "    best_model = grid_search.best_estimator_\n",
        "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = best_model.predict(X_test)\n",
        "    y_prob = best_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    # Evaluate the model\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    roc_auc = roc_auc_score(y_test, y_prob)\n",
        "\n",
        "    print(f\"Model Performance:\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1-Score: {f1:.4f}\")\n",
        "    print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
        "\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    return best_model\n",
        "\n",
        "def train_with_smote(X_train, y_train, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Train model with SMOTE for handling class imbalance\n",
        "    \"\"\"\n",
        "    print(\"Training with SMOTE for class imbalance handling...\")\n",
        "\n",
        "    # Create SMOTE pipeline\n",
        "    smote = SMOTE(random_state=42)\n",
        "    X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "    print(f\"Original training set shape: {X_train.shape}\")\n",
        "    print(f\"Balanced training set shape: {X_train_balanced.shape}\")\n",
        "    print(f\"Original fraud rate: {y_train.mean():.3f}\")\n",
        "    print(f\"Balanced fraud rate: {y_train_balanced.mean():.3f}\")\n",
        "\n",
        "    # Calculate scale_pos_weight for balanced dataset\n",
        "    scale_pos_weight = len(y_train_balanced[y_train_balanced == 0]) / len(y_train_balanced[y_train_balanced == 1])\n",
        "\n",
        "    # Train XGBoost model on balanced data\n",
        "    model = xgb.XGBClassifier(\n",
        "        n_estimators=200,\n",
        "        max_depth=8,\n",
        "        learning_rate=0.1,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        random_state=42,\n",
        "        eval_metric='auc',\n",
        "        scale_pos_weight=scale_pos_weight\n",
        "    )\n",
        "\n",
        "    model.fit(X_train_balanced, y_train_balanced)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_prob = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    # Evaluate the model\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    roc_auc = roc_auc_score(y_test, y_prob)\n",
        "\n",
        "    print(f\"SMOTE Model Performance:\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1-Score: {f1:.4f}\")\n",
        "    print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
        "\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    return model\n",
        "\n",
        "def train_ensemble_model(X_train, y_train, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Train an ensemble of models for better performance\n",
        "    \"\"\"\n",
        "    print(\"Training ensemble model (XGBoost + LightGBM)...\")\n",
        "\n",
        "    # Calculate scale_pos_weight for handling class imbalance\n",
        "    scale_pos_weight = len(y_train[y_train == 0]) / len(y_train[y_train == 1])\n",
        "\n",
        "    # Train XGBoost model\n",
        "    xgb_model = xgb.XGBClassifier(\n",
        "        n_estimators=200,\n",
        "        max_depth=8,\n",
        "        learning_rate=0.1,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        random_state=42,\n",
        "        eval_metric='auc',\n",
        "        scale_pos_weight=scale_pos_weight\n",
        "    )\n",
        "    xgb_model.fit(X_train, y_train)\n",
        "\n",
        "    # Train LightGBM model\n",
        "    lgb_model = lgb.LGBMClassifier(\n",
        "        n_estimators=200,\n",
        "        max_depth=8,\n",
        "        learning_rate=0.1,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        random_state=42,\n",
        "        objective='binary',\n",
        "        metric='auc'\n",
        "    )\n",
        "    lgb_model.fit(X_train, y_train)\n",
        "\n",
        "    # Get predictions from both models\n",
        "    xgb_pred = xgb_model.predict_proba(X_test)[:, 1]\n",
        "    lgb_pred = lgb_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    # Ensemble prediction (average)\n",
        "    ensemble_pred_proba = (xgb_pred + lgb_pred) / 2\n",
        "    ensemble_pred = (ensemble_pred_proba > 0.5).astype(int)\n",
        "\n",
        "    # Evaluate the ensemble model\n",
        "    accuracy = accuracy_score(y_test, ensemble_pred)\n",
        "    precision = precision_score(y_test, ensemble_pred)\n",
        "    recall = recall_score(y_test, ensemble_pred)\n",
        "    f1 = f1_score(y_test, ensemble_pred)\n",
        "    roc_auc = roc_auc_score(y_test, ensemble_pred_proba)\n",
        "\n",
        "    print(f\"Ensemble Model Performance:\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1-Score: {f1:.4f}\")\n",
        "    print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
        "\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, ensemble_pred))\n",
        "\n",
        "    # Return both models for later use\n",
        "    return xgb_model, lgb_model, ensemble_pred_proba\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to train the fraud detection model\n",
        "    \"\"\"\n",
        "    print(\"Starting Insurance Fraud Detection Model Training...\")\n",
        "\n",
        "    # Load and preprocess data\n",
        "    df, target_column = load_and_preprocess_data()\n",
        "    if df is None:\n",
        "        return\n",
        "\n",
        "    X, y, label_encoders = preprocess_data(df, target_column)\n",
        "\n",
        "    # Split the data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "\n",
        "    print(f\"Training set shape: {X_train.shape}\")\n",
        "    print(f\"Test set shape: {X_test.shape}\")\n",
        "    print(f\"Training target distribution:\\n{y_train.value_counts()}\")\n",
        "\n",
        "    # Scale the features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    # Save the scaler with feature names\n",
        "    scaler.feature_names_in_ = X.columns.tolist()\n",
        "    joblib.dump(scaler, 'scaler.pkl')\n",
        "\n",
        "    # Train the best model\n",
        "    best_model = train_best_model(X_train_scaled, y_train, X_test_scaled, y_test)\n",
        "\n",
        "    # Train model with SMOTE\n",
        "    smote_model = train_with_smote(X_train_scaled, y_train, X_test_scaled, y_test)\n",
        "\n",
        "    # Train ensemble model\n",
        "    xgb_model, lgb_model, ensemble_pred = train_ensemble_model(X_train_scaled, y_train, X_test_scaled, y_test)\n",
        "\n",
        "    # Save the best performing model\n",
        "    joblib.dump(best_model, 'fraud_detection_model.pkl')\n",
        "    print(\"\\nModel training completed!\")\n",
        "    print(\"Best model saved as 'fraud_detection_model.pkl'\")\n",
        "    print(\"Scaler saved as 'scaler.pkl'\")\n",
        "    print(\"Label encoders saved as 'label_encoders.pkl'\")\n",
        "\n",
        "    # Test with a sample from your test data\n",
        "    print(\"\\nTesting with your test data...\")\n",
        "    try:\n",
        "        test_df = pd.read_csv('/content/test_2021.csv')\n",
        "        print(f\"Loaded test data with shape: {test_df.shape}\")\n",
        "\n",
        "        # Preprocess test data\n",
        "        if 'claim_number' in test_df.columns:\n",
        "            test_df = test_df.drop('claim_number', axis=1)\n",
        "\n",
        "        if 'claim_date' in test_df.columns:\n",
        "            test_df['claim_date'] = pd.to_datetime(test_df['claim_date'], errors='coerce')\n",
        "            test_df['claim_year'] = test_df['claim_date'].dt.year\n",
        "            test_df['claim_month'] = test_df['claim_date'].dt.month\n",
        "            test_df['claim_day'] = test_df['claim_date'].dt.day\n",
        "            test_df = test_df.drop('claim_date', axis=1)\n",
        "\n",
        "        # Apply label encoding to categorical columns\n",
        "        for col, encoder in label_encoders.items():\n",
        "            if col in test_df.columns:\n",
        "                # Handle unseen labels by using the first label\n",
        "                test_df[col] = test_df[col].apply(\n",
        "                    lambda x: x if str(x) in encoder.classes_ else encoder.classes_[0]\n",
        "                )\n",
        "                test_df[col] = encoder.transform(test_df[col].astype(str))\n",
        "\n",
        "        # Ensure all features are present and in the correct order\n",
        "        expected_features = scaler.feature_names_in_\n",
        "        missing_features = set(expected_features) - set(test_df.columns)\n",
        "\n",
        "        # Add missing features with default values (mean from training)\n",
        "        for feature in missing_features:\n",
        "            test_df[feature] = 0  # Default value for missing features\n",
        "\n",
        "        # Reorder columns to match training order\n",
        "        test_df = test_df.reindex(columns=expected_features, fill_value=0)\n",
        "\n",
        "        # Scale the test data\n",
        "        test_scaled = scaler.transform(test_df)\n",
        "\n",
        "        # Make predictions\n",
        "        test_predictions = best_model.predict(test_scaled)\n",
        "        test_probabilities = best_model.predict_proba(test_scaled)[:, 1]\n",
        "\n",
        "        print(f\"Test predictions: {test_predictions[:10]} (first 10)\")\n",
        "        print(f\"Test probabilities: {test_probabilities[:10]} (first 10)\")\n",
        "\n",
        "        # Show some detailed results\n",
        "        for i in range(min(5, len(test_predictions))):\n",
        "            print(f\"Sample {i+1}: Prediction={test_predictions[i]}, Probability={test_probabilities[i]:.3f}\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(\"Test data file not found at /content/test_2021.csv\")\n",
        "\n",
        "# Run the main training function\n",
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-KU1Ey9lGtuB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OwKfTTtu8RMF"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install ipywidgets\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "from IPython.display import display, clear_output\n",
        "import ipywidgets as widgets\n",
        "\n",
        "# Load the trained model, scaler, and label encoders\n",
        "try:\n",
        "    model = joblib.load('fraud_detection_model.pkl')\n",
        "    scaler = joblib.load('scaler.pkl')\n",
        "    label_encoders = joblib.load('label_encoders.pkl')\n",
        "    print(\"‚úÖ Models loaded successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading models: {e}\")\n",
        "    print(\"Make sure you have run the training script first and the model files exist.\")\n",
        "\n",
        "# Define categorical options\n",
        "gender_options = ['M', 'F']\n",
        "marital_status_options = [0.0, 1.0]\n",
        "living_status_options = ['Own', 'Rent']\n",
        "claim_day_options = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
        "accident_site_options = ['Highway', 'Local', 'Parking Lot']\n",
        "channel_options = ['Broker', 'Online', 'Phone']\n",
        "vehicle_category_options = ['Compact', 'Large', 'Medium']\n",
        "vehicle_color_options = ['black', 'blue', 'gray', 'other', 'red', 'silver', 'white']\n",
        "\n",
        "# Create input widgets\n",
        "age_of_driver = widgets.IntSlider(value=40, min=18, max=80, description='Age of Driver:')\n",
        "gender = widgets.Dropdown(options=gender_options, value='M', description='Gender:')\n",
        "marital_status = widgets.Dropdown(options=marital_status_options, value=1.0, description='Marital Status:')\n",
        "safty_rating = widgets.IntSlider(value=75, min=1, max=100, description='Safety Rating:')\n",
        "annual_income = widgets.IntText(value=35000, description='Annual Income ($):')\n",
        "high_education_ind = widgets.Dropdown(options=[0, 1], value=1, description='High Education:')\n",
        "address_change_ind = widgets.Dropdown(options=[0, 1], value=0, description='Address Change:')\n",
        "living_status = widgets.Dropdown(options=living_status_options, value='Own', description='Living Status:')\n",
        "zip_code = widgets.IntText(value=50001, description='ZIP Code:')\n",
        "claim_day_of_week = widgets.Dropdown(options=claim_day_options, value='Monday', description='Claim Day:')\n",
        "accident_site = widgets.Dropdown(options=accident_site_options, value='Local', description='Accident Site:')\n",
        "past_num_of_claims = widgets.IntSlider(value=0, min=0, max=10, description='Past Claims:')\n",
        "witness_present_ind = widgets.Dropdown(options=[0.0, 1.0], value=1.0, description='Witness:')\n",
        "liab_prct = widgets.FloatSlider(value=0.5, min=0.0, max=1.0, step=0.01, description='Liability %:')\n",
        "channel = widgets.Dropdown(options=channel_options, value='Broker', description='Channel:')\n",
        "policy_report_filed_ind = widgets.Dropdown(options=[0, 1], value=1, description='Policy Filed:')\n",
        "claim_est_payout = widgets.IntText(value=5000, description='Claim Payout ($):')\n",
        "age_of_vehicle = widgets.FloatSlider(value=5.0, min=0.0, max=30.0, step=0.1, description='Vehicle Age:')\n",
        "vehicle_category = widgets.Dropdown(options=vehicle_category_options, value='Medium', description='Category:')\n",
        "vehicle_price = widgets.IntText(value=20000, description='Vehicle Price ($):')\n",
        "vehicle_color = widgets.Dropdown(options=vehicle_color_options, value='white', description='Color:')\n",
        "vehicle_weight = widgets.FloatText(value=3000.0, description='Vehicle Weight (lbs):')\n",
        "claim_year = widgets.IntSlider(value=2022, min=2015, max=2025, description='Claim Year:')\n",
        "claim_month = widgets.IntSlider(value=6, min=1, max=12, description='Claim Month:')\n",
        "claim_day = widgets.IntSlider(value=15, min=1, max=31, description='Claim Day:')\n",
        "\n",
        "# Create prediction output area\n",
        "output = widgets.Output()\n",
        "\n",
        "def predict_fraud(button):\n",
        "    with output:\n",
        "        clear_output()\n",
        "        print(\"üîç Making prediction...\")\n",
        "\n",
        "        # Get form data\n",
        "        data = {\n",
        "            \"age_of_driver\": age_of_driver.value,\n",
        "            \"gender\": gender.value,\n",
        "            \"marital_status\": float(marital_status.value),\n",
        "            \"safty_rating\": safty_rating.value,\n",
        "            \"annual_income\": float(annual_income.value),\n",
        "            \"high_education_ind\": high_education_ind.value,\n",
        "            \"address_change_ind\": address_change_ind.value,\n",
        "            \"living_status\": living_status.value,\n",
        "            \"zip_code\": zip_code.value,\n",
        "            \"claim_day_of_week\": claim_day_of_week.value,\n",
        "            \"accident_site\": accident_site.value,\n",
        "            \"past_num_of_claims\": past_num_of_claims.value,\n",
        "            \"witness_present_ind\": float(witness_present_ind.value),\n",
        "            \"liab_prct\": float(liab_prct.value),\n",
        "            \"channel\": channel.value,\n",
        "            \"policy_report_filed_ind\": policy_report_filed_ind.value,\n",
        "            \"claim_est_payout\": float(claim_est_payout.value),\n",
        "            \"age_of_vehicle\": float(age_of_vehicle.value),\n",
        "            \"vehicle_category\": vehicle_category.value,\n",
        "            \"vehicle_price\": float(vehicle_price.value),\n",
        "            \"vehicle_color\": vehicle_color.value,\n",
        "            \"vehicle_weight\": float(vehicle_weight.value),\n",
        "            \"claim_year\": claim_year.value,\n",
        "            \"claim_month\": claim_month.value,\n",
        "            \"claim_day\": claim_day.value\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            # Create DataFrame from input\n",
        "            input_df = pd.DataFrame([data])\n",
        "\n",
        "            # Apply label encoding to categorical columns\n",
        "            for col, encoder in label_encoders.items():\n",
        "                if col in input_df.columns:\n",
        "                    # Handle unseen labels by using the first label\n",
        "                    input_df[col] = input_df[col].apply(\n",
        "                        lambda x: x if str(x) in encoder.classes_ else encoder.classes_[0]\n",
        "                    )\n",
        "                    input_df[col] = encoder.transform(input_df[col].astype(str))\n",
        "\n",
        "            # Ensure all features are present and in the correct order\n",
        "            expected_features = scaler.feature_names_in_\n",
        "            missing_features = set(expected_features) - set(input_df.columns)\n",
        "\n",
        "            # Add missing features with default values (mean from training)\n",
        "            for feature in missing_features:\n",
        "                input_df[feature] = 0  # Default value for missing features\n",
        "\n",
        "            # Reorder columns to match training order\n",
        "            input_df = input_df.reindex(columns=expected_features, fill_value=0)\n",
        "\n",
        "            # Scale the features\n",
        "            input_scaled = scaler.transform(input_df)\n",
        "\n",
        "            # Make prediction\n",
        "            prediction = model.predict(input_scaled)[0]\n",
        "            probability = model.predict_proba(input_scaled)[0][1]  # Probability of fraud\n",
        "\n",
        "            print(\"üìä Prediction Results:\")\n",
        "            print(f\"Status: {'‚ö†Ô∏è FRAUDULENT' if prediction == 1 else '‚úÖ GENUINE'}\")\n",
        "            print(f\"Probability of Fraud: {probability:.3f} ({probability*100:.1f}%)\")\n",
        "            print(f\"Confidence: {(1-probability)*100:.1f}% for genuine, {probability*100:.1f}% for fraud\")\n",
        "\n",
        "            if prediction == 1:\n",
        "                print(\"\\nüö® ALERT: This claim is flagged as potentially fraudulent!\")\n",
        "                print(\"üí° Recommendation: Investigate this claim further\")\n",
        "            else:\n",
        "                print(\"\\n‚úÖ This claim appears to be genuine.\")\n",
        "                print(\"‚úÖ Recommendation: Standard processing can proceed\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error making prediction: {e}\")\n",
        "\n",
        "# Create the prediction button\n",
        "predict_button = widgets.Button(description=\"üîç Check for Fraud\")\n",
        "predict_button.on_click(predict_fraud)\n",
        "\n",
        "# Create the UI layout\n",
        "form_items = [\n",
        "    widgets.HTML(\"<h2>üöó Insurance Fraud Detection System</h2>\"),\n",
        "    widgets.HTML(\"<p>Enter claim details below and click 'Check for Fraud' to get prediction</p>\"),\n",
        "    widgets.HBox([age_of_driver, gender]),\n",
        "    widgets.HBox([marital_status, safty_rating]),\n",
        "    widgets.HBox([annual_income, high_education_ind]),\n",
        "    widgets.HBox([address_change_ind, living_status]),\n",
        "    widgets.HBox([zip_code, claim_day_of_week]),\n",
        "    widgets.HBox([accident_site, past_num_of_claims]),\n",
        "    widgets.HBox([witness_present_ind, liab_prct]),\n",
        "    widgets.HBox([channel, policy_report_filed_ind]),\n",
        "    widgets.HBox([claim_est_payout, age_of_vehicle]),\n",
        "    widgets.HBox([vehicle_category, vehicle_price]),\n",
        "    widgets.HBox([vehicle_color, vehicle_weight]),\n",
        "    widgets.HBox([claim_year, claim_month, claim_day]),\n",
        "    predict_button,\n",
        "    output\n",
        "]\n",
        "\n",
        "form = widgets.VBox(form_items)\n",
        "print(\"üéØ Interactive Fraud Detection UI Ready:\")\n",
        "display(form)\n",
        "\n",
        "# Create a sample test button\n",
        "def test_with_sample(button):\n",
        "    with output:\n",
        "        clear_output()\n",
        "        print(\"Testing with sample data from your test dataset...\")\n",
        "\n",
        "        try:\n",
        "            # Load test data\n",
        "            test_df = pd.read_csv('/content/test_2021.csv')\n",
        "\n",
        "            # Select a random sample\n",
        "            sample_idx = np.random.randint(0, len(test_df))\n",
        "            sample_row = test_df.iloc[sample_idx]\n",
        "\n",
        "            print(f\"Testing with sample {sample_idx+1} from test dataset:\")\n",
        "            print(f\"Claim Number: {sample_row['claim_number']}\")\n",
        "            print(f\"Claim Date: {sample_row['claim_date']}\")\n",
        "            print(f\"Claim Payout: ${sample_row['claim_est_payout']}\")\n",
        "\n",
        "            # Prepare data for prediction\n",
        "            data = {\n",
        "                \"age_of_driver\": int(sample_row['age_of_driver']),\n",
        "                \"gender\": str(sample_row['gender']),\n",
        "                \"marital_status\": float(sample_row['marital_status']),\n",
        "                \"safty_rating\": int(sample_row['safty_rating']),\n",
        "                \"annual_income\": float(sample_row['annual_income']),\n",
        "                \"high_education_ind\": int(sample_row['high_education_ind']),\n",
        "                \"address_change_ind\": int(sample_row['address_change_ind']),\n",
        "                \"living_status\": str(sample_row['living_status']),\n",
        "                \"zip_code\": int(sample_row['zip_code']),\n",
        "                \"claim_day_of_week\": str(sample_row['claim_day_of_week']),\n",
        "                \"accident_site\": str(sample_row['accident_site']),\n",
        "                \"past_num_of_claims\": int(sample_row['past_num_of_claims']),\n",
        "                \"witness_present_ind\": float(sample_row['witness_present_ind']),\n",
        "                \"liab_prct\": float(sample_row['liab_prct']),\n",
        "                \"channel\": str(sample_row['channel']),\n",
        "                \"policy_report_filed_ind\": int(sample_row['policy_report_filed_ind']),\n",
        "                \"claim_est_payout\": float(sample_row['claim_est_payout']),\n",
        "                \"age_of_vehicle\": float(sample_row['age_of_vehicle']),\n",
        "                \"vehicle_category\": str(sample_row['vehicle_category']),\n",
        "                \"vehicle_price\": float(sample_row['vehicle_price']),\n",
        "                \"vehicle_color\": str(sample_row['vehicle_color']),\n",
        "                \"vehicle_weight\": float(sample_row['vehicle_weight']),\n",
        "                \"claim_year\": 2021,  # Default for test data\n",
        "                \"claim_month\": 1,    # Default for test data\n",
        "                \"claim_day\": 1       # Default for test data\n",
        "            }\n",
        "\n",
        "            # Create DataFrame from input\n",
        "            input_df = pd.DataFrame([data])\n",
        "\n",
        "            # Apply label encoding to categorical columns\n",
        "            for col, encoder in label_encoders.items():\n",
        "                if col in input_df.columns:\n",
        "                    # Handle unseen labels by using the first label\n",
        "                    input_df[col] = input_df[col].apply(\n",
        "                        lambda x: x if str(x) in encoder.classes_ else encoder.classes_[0]\n",
        "                    )\n",
        "                    input_df[col] = encoder.transform(input_df[col].astype(str))\n",
        "\n",
        "            # Ensure all features are present and in the correct order\n",
        "            expected_features = scaler.feature_names_in_\n",
        "            missing_features = set(expected_features) - set(input_df.columns)\n",
        "\n",
        "            # Add missing features with default values (mean from training)\n",
        "            for feature in missing_features:\n",
        "                input_df[feature] = 0  # Default value for missing features\n",
        "\n",
        "            # Reorder columns to match training order\n",
        "            input_df = input_df.reindex(columns=expected_features, fill_value=0)\n",
        "\n",
        "            # Scale the features\n",
        "            input_scaled = scaler.transform(input_df)\n",
        "\n",
        "            # Make prediction\n",
        "            prediction = model.predict(input_scaled)[0]\n",
        "            probability = model.predict_proba(input_scaled)[0][1]  # Probability of fraud\n",
        "\n",
        "            print(\"\\nüìä Prediction Results:\")\n",
        "            print(f\"Status: {'‚ö†Ô∏è FRAUDULENT' if prediction == 1 else '‚úÖ GENUINE'}\")\n",
        "            print(f\"Probability of Fraud: {probability:.3f} ({probability*100:.1f}%)\")\n",
        "            print(f\"Confidence: {(1-probability)*100:.1f}% for genuine, {probability*100:.1f}% for fraud\")\n",
        "\n",
        "            if prediction == 1:\n",
        "                print(\"\\nüö® ALERT: This claim is flagged as potentially fraudulent!\")\n",
        "            else:\n",
        "                print(\"\\n‚úÖ This claim appears to be genuine.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error testing with sample: {e}\")\n",
        "\n",
        "# Create sample test button\n",
        "sample_button = widgets.Button(description=\"üé≤ Test with Sample Data\")\n",
        "sample_button.on_click(test_with_sample)\n",
        "\n",
        "# Add sample button to the form\n",
        "form_items_with_sample = [\n",
        "    widgets.HTML(\"<h2>üöó Insurance Fraud Detection System</h2>\"),\n",
        "    widgets.HTML(\"<p>Enter claim details below and click 'Check for Fraud' to get prediction</p>\"),\n",
        "    widgets.HBox([age_of_driver, gender]),\n",
        "    widgets.HBox([marital_status, safty_rating]),\n",
        "    widgets.HBox([annual_income, high_education_ind]),\n",
        "    widgets.HBox([address_change_ind, living_status]),\n",
        "    widgets.HBox([zip_code, claim_day_of_week]),\n",
        "    widgets.HBox([accident_site, past_num_of_claims]),\n",
        "    widgets.HBox([witness_present_ind, liab_prct]),\n",
        "    widgets.HBox([channel, policy_report_filed_ind]),\n",
        "    widgets.HBox([claim_est_payout, age_of_vehicle]),\n",
        "    widgets.HBox([vehicle_category, vehicle_price]),\n",
        "    widgets.HBox([vehicle_color, vehicle_weight]),\n",
        "    widgets.HBox([claim_year, claim_month, claim_day]),\n",
        "    widgets.HBox([predict_button, sample_button]),\n",
        "    output\n",
        "]\n",
        "\n",
        "form_with_sample = widgets.VBox(form_items_with_sample)\n",
        "print(\"üéØ Interactive Fraud Detection UI with Sample Testing:\")\n",
        "display(form_with_sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "tKCRIz9qFkG1",
        "outputId": "977afee3-4914-4060-c2c8-93215e4b11d6"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_92d7c6a4-d486-4f57-bb01-2ca76043ce1c\", \"fraud_detection_model.pkl\", 428781)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_e358bd75-b81b-4681-84a1-099e6678f049\", \"scaler.pkl\", 1631)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_cf05ad4b-5182-4ad0-97cc-328925e536b5\", \"label_encoders.pkl\", 2067)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Downloaded models:\n",
            "- fraud_detection_model.pkl (XGBoost model)\n",
            "- scaler.pkl (Feature scaler)\n",
            "- label_encoders.pkl (Categorical encoders)\n"
          ]
        }
      ],
      "source": [
        "import joblib\n",
        "from google.colab import files\n",
        "\n",
        "# Download the best performing model (XGBoost with hyperparameter tuning)\n",
        "files.download('fraud_detection_model.pkl')\n",
        "files.download('scaler.pkl')\n",
        "files.download('label_encoders.pkl')\n",
        "\n",
        "print(\"‚úÖ Downloaded models:\")\n",
        "print(\"- fraud_detection_model.pkl (XGBoost model)\")\n",
        "print(\"- scaler.pkl (Feature scaler)\")\n",
        "print(\"- label_encoders.pkl (Categorical encoders)\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
